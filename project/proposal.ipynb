{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision: I will be making a final project based around the topic of variational autoencoders, or VAES. VAES are a technology in the field of realistic image autogeneration, and additionally allow us to develop models which are capable of determining which images are similar to one another. I hope to use this technology on the Fashion-MNIST dataset to be able to autogenerate realistic 28x28 grayscaled images of various articles of clothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background: Essentially, VAES alter the traditional technology of image autoencoders, which take images, compress them into a latent vector space, and then decode them back to an image of identical dimensions. However, instead of compressing the input image into a single code, VAES compress the image into the parameters of a representative distribution, which is then sampled from in order for the decoder to reconstruct the input.\n",
    "    Chapter 8.4 of Chollet's Deep Learning with Python Notebooks provides an example of a VAE created around the traditional MNIST dataset, which is comprised of 28x28 representations of handwritten digits. I'll be using this example to create a VAE that will operate on Keras' Fashion-MNIST dataset. I had originally planned to use VAES on Keras' CIFAR10 dataset, which contains 32x32 full color images sorted into 10 categories, but upon reading a report from several University of Berkeley California students on their attempt at a VAE built to process CIFAR10, I decided to switch to Fashion-MNIST, with which I believe I can actually achieve some level of success. The students from Berkeley's network was not capable of generating realistic images, and their research showed that even state of the art networks struggle with the CIFAR10 dataset. Their report is linked here: https://bcourses.berkeley.edu/files/70257161/download?download_frd=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation: I plan to begin from the MNIST example in Chapter 8.4 of Chollet's Deep Learning with Python Notebooks. Fashion-MNIST is designed to be a direct swap for MNIST in terms of dataset and parameter sizes, so while I expect this swap to have poor performance initially without more fine tuning, the swap itself shouldn't be difficult. From there, I will fine-tune the VAE and subsequent neural network in order to produce improved performance on the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: My overall goal is to develop a model than can autogenerate images well, or at least reach the point that Chollet does in the Python Notebook for 8.4. To judge how well the network is autogenerating images, I'll be using the same three standards that the students from Berkeley did on their VAE: the likelihood of the training and test data using the trained VAE, the rather unquantitative metric of the visual quality of the reconstructed and autogenerated images, and the classification accuracy for unlabeled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implications: In terms of social and ethical implications, VAES and image autogeneration have some rather foreboding potential. If one could use a network to autogenerate any type of image they'd like, generating a realistic looking but false image in order to blackmail or slander would not be remotely difficult. Whenever technology is used to tell a lie that is difficult to discern from the truth, we should always be careful about how far we're willing to trust it to bring us. Generating 28x28 images that look like clothing is quite harmless, but as this technology improves more sinister utilities could be unlocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
