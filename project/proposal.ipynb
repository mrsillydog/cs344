{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be making a final project based around the topic of variational autoencoders, or VAES. VAES are a technology in the field of realistic image autogeneration, and additionally allow us to develop models which are capable of determining which images are similar to one another. Essentially, they alter the traditional technology of image autoencoders, which take images, compress them into a latent vector space, and then decode them back to an image of identical dimensions. However, instead of compressing the input image into a single code, VAES compress the image into the parameters of a representative distribution, which is then sampled from for the decoder to to reconstruct the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 8.4 of Chollet's Deep Learning with Python Notebooks provides an example of a VAE created around the traditional MNIST dataset, which is comprised of 28x28 representations of handwritten digits. It's also quite easy to find examples of VAEs created to be used with the more complicated Fashion-MNIST dataset, which containts 28x28 representations of various types of clothing items. I'll be using these examples to create a VAE that will operate on keras.dataset's CIFAR10 dataset. This dataset bears some similarities to MNIST and Fashion-MNIST; primarily, it contains images assigned to 1 of 10 categories, just like MNIST and Fashion-MNIST. However, it's also more complicated than the other two datasets; first, instead of containing grayscale images of numbers or clothing images, it contains full color images of various animals and vehicles. Additionally, the dimensions of these images are 32x32, rather than 28x28. Overall, it is a more complex dataset, but I'm hopeful for my VAE's success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My overall goal is to develop a model than can autogenerate images well, or at least reach the point that Chollet does in the Python Notebook for 8.4; I don't have a good enough understanding of VAE's to understand how to quantify how well my model autogenerates images yet, but this should be cleared up in the proposal update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
