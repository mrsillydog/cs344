8.4
a. K-fold validation allows us to reliably evaluate our model. Since we have so few data points, our validation scores could have a great deal of variance depending on which data points we choose to be in the validation set. K-fold validation removes this variance by instantiating K identical models using K-1/Kths of the total data as the training set and using the final 1/Kth as the validation set. By averaging the validation scores, we can come up with a more accurate overall score for the overall model.
b. It would simply slow down the neural network's ability to learn. Feeding in data values with wildly different ranges might result in the parameters being altered by overly large amounts, since the current parameters might be really far off from the data in a completely different range on each iteration.
c. I do agree. Since big networks with lots of units on many layers tend to hypertune parameters, simply because there's more processing that goes on, using a small amount of data on a big network seems like a great way to hypertune to the more prominent particularities of your small amount of data. Using a small network instead would result in less precise tuning and therefore reduce this overfitting, like he says.
d. I've been trying, unsuccessfully, to get this code running for about 6 hours now. My best guess is that none of the other options perform better; 3 layers because it overfits, 1 layer because it doesn't fit will enough, wider layers because they overfit, narrower layers because they underfit.